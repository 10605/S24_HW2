{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81bb94b1-d42d-4166-adec-a184fcbcea1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# CMU 10-605/10-805 auto-graded notebook\n",
    "\n",
    "Before you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel → Restart) and then **run all cells** (in the menubar, select Cell → Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f0e217-2a8c-4b8a-beb1-e5c030db0de5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b82c19ec-aec6-452c-91db-26e77b60fbcb",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "# CMU 10405/605 Machine Learning with Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9565b38-b67d-43a3-a720-ba56194b35d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Homework 2 - Part 2:  PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aae1676e-8836-4ff2-9730-1a1dbabe3a96",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe60cd9540617cc2e184126ec775bb7a",
     "grade": false,
     "grade_id": "collaborators",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Who did you collaborate with on this assignment? \n",
    "# if no one, collaborators should contain an empty string,\n",
    "# else list your collaborators below\n",
    "\n",
    "# collaborators = [\"\"]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2489c2dd-0682-4ec9-8fef-e2178b5bd93b",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dccedd0e8702e7a99d0ea08f2fa7921",
     "grade": true,
     "grade_id": "test_collaborators",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    collaborators\n",
    "except:\n",
    "    raise AssertionError(\"you did not list your collaborators, if any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a5ae818-dbaf-4ee8-b7e8-206773b9d0ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6816d173-d0bb-4b1f-bf69-71c5dd8a1a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOU CAN MOST LIKELY IGNORE THIS CELL. This is only of use for running this notebook locally.\n",
    "\n",
    "# THIS CELL DOES NOT NEED TO BE RUN ON DATABRICKS. \n",
    "# Note that Databricks already creates a SparkContext for you, so this cell can be skipped.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"hw\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"False\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"spark context started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e392bc1e-1d1a-440d-a130-8f335514fd89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Principle Component Analysis\n",
    "\n",
    "In this part we will dive into exploratory analysis of neuroscience data, specifically using principal component analysis (PCA) and feature-based aggregation. We will use a dataset of light-sheet imaging recorded by the [Ahrens Lab](http://www.janelia.org/lab/ahrens-lab) at Janelia Research Campus.\n",
    "\n",
    "Our dataset is generated by studying the movement of a larval [zebrafish](http://en.wikipedia.org/wiki/Zebrafish), an animal that is especially useful in neuroscience because it is transparent, making it possible to record activity over its entire brain using a technique called [light-sheet microscopy](http://en.wikipedia.org/wiki/Light_sheet_fluorescence_microscopy).   Specifically, we'll work with time-varying images containing patterns of the zebrafish's neural activity as it is presented with a moving visual pattern.   Different stimuli induce different patterns across the brain, and we can use exploratory analyses to identify these patterns.  Read [\"Mapping brain activity at scale with cluster computing\"](https://www.janelia.org/sites/default/files/Library/nmeth.3041.pdf) for more information about these kinds of analyses.\n",
    "\n",
    "During this lab you will learn about PCA, and then compare and contrast different exploratory analyses of the same data set to identify which neural patterns they best highlight.\n",
    "\n",
    "## This section will cover:\n",
    "\n",
    "*  *Part 1:* Work through the steps of PCA on a sample dataset\n",
    " * *Visualization 1:* Two-dimensional Gaussians\n",
    "\n",
    "*  *Part 2:* Write a PCA function and evaluate PCA on sample datasets\n",
    " * *Visualization 2:* PCA projection\n",
    " * *Visualization 3:* Three-dimensional data\n",
    " * *Visualization 4:* 2D representation of 3D data\n",
    "\n",
    "*  *Part 3:* Parse, inspect, and preprocess neuroscience data then perform PCA\n",
    " * *Visualization 5:* Pixel intensity\n",
    " * *Visualization 6:* Normalized data\n",
    " * *Visualization 7:* Top two components as images\n",
    " * *Visualization 8:* Top two components as one image\n",
    "\n",
    "*  *Part 4:* Perform feature-based aggregation followed by PCA\n",
    " * *Visualization 9:* Top two components by time\n",
    " * *Visualization 10:* Top two components by direction\n",
    "\n",
    "*  *Part 5:* Perform random projections \n",
    " * *Visualization 11:* Plot the top two eigenvectors \n",
    "> Note that, for reference, you can look up the details of:\n",
    "> * the relevant Spark methods in [PySpark's DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n",
    "> * the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc7510ee-d493-4ee1-b9bb-6e404b2195cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 1: Work through the steps of PCA on a sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "161929c3-82e8-4f05-a237-6388eb17ca39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 1: Two-dimensional Gaussians\n",
    "\n",
    "Principal Component Analysis, or PCA, is a strategy for dimensionality reduction. To better understand PCA, we'll work with synthetic data generated by sampling from the [two-dimensional Gaussian distribution](http://en.wikipedia.org/wiki/Multivariate_normal_distribution).  This distribution takes as input the mean and variance of each dimension, as well as the covariance between the two dimensions.\n",
    "\n",
    "In our visualizations below, we will specify the mean of each dimension to be 50 and the variance along each dimension to be 1.  We will explore two different values for the covariance: 0 and 0.9. When the covariance is zero, the two dimensions are uncorrelated, and hence the data looks spherical.  In contrast, when the covariance is 0.9, the two dimensions are strongly (positively) correlated and thus the data is non-spherical.  As we'll see in Parts 1 and 2, the non-spherical data is amenable to dimensionality reduction via PCA, while the spherical data is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eed1e00c-8bc4-412e-9c58-4b19010f6531",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def prepare_plot(xticks, yticks, figsize=(10.5, 6), hide_labels=False, grid_color='#999999',\n",
    "                 grid_width=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hide_labels: axis.set_ticklabels([])\n",
    "    plt.grid(color=grid_color, linewidth=grid_width, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "def create_2D_gaussian(mn, variance, cov, n):\n",
    "    \"\"\"Randomly sample points from a two-dimensional Gaussian distribution\"\"\"\n",
    "    np.random.seed(142)\n",
    "    return np.random.multivariate_normal(np.array([mn, mn]), np.array([[variance, cov], [cov, variance]]), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc61a85b-0241-4a0e-8cbf-9de0a1c82980",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_random = create_2D_gaussian(mn=50, variance=1, cov=0, n=100)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45, 54.5), ax.set_ylim(45, 54.5)\n",
    "plt.scatter(data_random[:,0], data_random[:,1], s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e511d7f-7416-484b-bba5-6da42947ff18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_correlated = create_2D_gaussian(mn=50, variance=1, cov=.9, n=100)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.scatter(data_correlated[:,0], data_correlated[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60f42231-8ad1-47db-b526-d0423aabe85a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (1a) Interpreting PCA\n",
    "\n",
    "PCA can be interpreted as identifying the \"directions\" along which the data vary the most. In the first step of PCA, we must first center our data.  Working with our correlated dataset, first compute the mean of each feature (column) in the dataset.  Then for each observation, modify the features by subtracting their corresponding mean, to create a zero mean dataset.\n",
    "\n",
    "> Note:\n",
    "> * `correlated_data` is an RDD of NumPy arrays.\n",
    "> * This allows us to perform certain operations more succinctly.\n",
    "> * For example, we can sum the columns of our dataset using `correlated_data.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb54eb61-c026-42a3-b373-f2f6fe934731",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "822ef247d9225db61fac4a57c5c7e397",
     "grade": false,
     "grade_id": "answer_interpretePCA1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "correlated_data = sc.parallelize(data_correlated)\n",
    "\n",
    "def generate_zero_mean_dataset(dataset):\n",
    "    \"\"\"\n",
    "    TODO: To obtain the zero mean dataset, \n",
    "            1) compute the mean of each feature\n",
    "            2) subtract it from the original feature\n",
    "    \n",
    "    :param dataset: the dataset being analyzed\n",
    "    :return: (mean of features, zero mean dataset)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "mean_correlated, correlated_data_zero_mean = generate_zero_mean_dataset(correlated_data)\n",
    "\n",
    "\n",
    "print(mean_correlated)\n",
    "print(correlated_data.take(1))\n",
    "print(correlated_data_zero_mean.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d3606b8-40df-41cb-a4ef-4df0c6f3360c",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57f0af873c1b603614e100e2083e45c9",
     "grade": true,
     "grade_id": "test_interpretePCA1a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Interpreting PCA (1a)\n",
    "from nose.tools import assert_true, assert_equal\n",
    "assert_true(np.allclose(mean_correlated, [49.95739037, 49.97180477], atol=1e-2),\n",
    "                'incorrect value for mean_correlated')\n",
    "assert_true(np.allclose(correlated_data_zero_mean.take(1)[0], [-0.28561917, 0.10351492], atol=1e-2),\n",
    "                'incorrect value for correlated_data_zero_mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0fa7702-4c1f-4344-8cbc-38ac5622e8cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (1b) Sample covariance matrix\n",
    "\n",
    "We are now ready to compute the sample covariance matrix. If we define \\\\(\\scriptsize \\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\\\) as the zero mean data matrix, then the sample covariance matrix is defined as: \\\\[ \\mathbf{C}_{\\mathbf X} = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X} \\,.\\\\]\n",
    "\n",
    "To compute this matrix, compute the outer product of each data point, add together these outer products, and divide by the number of data points. The data are two dimensional, so the resulting covariance matrix should be a 2x2 matrix.\n",
    "\n",
    "> Note:\n",
    "> * [np.outer()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html) can be used to calculate the outer product of two NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f610f1c-01e2-4920-a26e-70c7713580d3",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e17fa153b9e97b527202b85a8395757d",
     "grade": false,
     "grade_id": "answer_covMatrix1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_covariance_matrix(zero_mean_dataset):\n",
    "    \"\"\"\n",
    "    TODO: Compute the covariance matrix using outer products\n",
    "            1) Compute the outer product of each data point\n",
    "            2) Add together these outer products\n",
    "            3) Divide by the number of data points\n",
    "            \n",
    "    :param zero_mean_dataset: zero mean dataset \n",
    "    :return: (covariance matrix)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "\n",
    "correlated_cov = compute_covariance_matrix(correlated_data_zero_mean)\n",
    "print(correlated_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2001b307-f140-4e36-ad8b-9f1f9c5e267d",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18da622a665bec45ab2746377a7da8af",
     "grade": true,
     "grade_id": "test_covMatrix1b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Sample covariance matrix (1b)\n",
    "cov_result = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
    "assert_true(np.allclose(cov_result, correlated_cov, atol=1e-2), 'incorrect value for correlated_cov')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0d09d63-b580-4862-8a72-3766156facc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (1c) Covariance Function\n",
    "\n",
    "Next, use the expressions above to write a function to compute the sample covariance matrix for an arbitrary `data` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8986f2a-dd60-4a29-8996-93a7056e5657",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "531fb96a784ca85928ed72e523486bde",
     "grade": false,
     "grade_id": "answer_covFunction1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_covariance(data):\n",
    "    \"\"\"\n",
    "    TODO: Compute the covariance matrix for an arbitrary RDD\n",
    "            1) Compute the zero mean data\n",
    "            2) Compute covariance\n",
    "        HINT: Utilize the functions implemented in 1(a) and 1(b)\n",
    "        \n",
    "    :param data: an 'RDD' consisting of NumPy arrays\n",
    "    :return: (covariance matrix)\n",
    "        WHERE\n",
    "           covariance matrix - is a np.ndarray where the number of rows and columns \n",
    "                               both equal the length of the arrays in the input `RDD`\n",
    "    \"\"\"   \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    raise NotImplementedError() \n",
    "\n",
    "correlated_cov_auto= estimate_covariance(correlated_data)\n",
    "print(correlated_cov_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b03682ce-505f-40a6-9485-3f3cd6356af0",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fe508a64b160956ef09a13a8acd529f",
     "grade": true,
     "grade_id": "test_covFunction1c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Covariance function (1c)\n",
    "correct_cov = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
    "assert_true(np.allclose(correct_cov, correlated_cov_auto, atol=1e-2),\n",
    "                'incorrect value for correlated_cov_auto')\n",
    "\n",
    "test_data = np.array([[0,1,2,3], [4,5,6,7], [8,9,10,11], [12,13,14,15]])\n",
    "cov_test_data = sc.parallelize(test_data)\n",
    "correct_test_cov = [[20., 20., 20., 20.],\n",
    "                    [ 20.,  20.,  20.,  20.],\n",
    "                    [ 20.,  20.,  20.,  20.],\n",
    "                    [ 20.,  20.,  20.,  20.]]\n",
    "assert_true(np.allclose(correct_test_cov, estimate_covariance(cov_test_data), atol=1e-2), 'incorrect value returned by estimate_covariance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e54ed43-61f6-457a-993e-756aeed2d4af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (1d) Eigendecomposition\n",
    "\n",
    "Now that we've computed the sample covariance matrix, we can use it to find directions of maximal variance in the data.  Specifically, we can perform an eigendecomposition of this matrix to find its eigenvalues and eigenvectors.  The \\\\(\\scriptsize d \\\\) eigenvectors of the covariance matrix give us the directions of maximal variance, and are often called the \"principal components.\"  The associated eigenvalues are the variances in these directions.  In particular, the eigenvector corresponding to the largest eigenvalue is the direction of maximal variance (this is sometimes called the \"top\" eigenvector). Eigendecomposition of a \\\\(\\scriptsize d \\times d \\\\) covariance matrix has a (roughly) cubic runtime complexity with respect to \\\\(\\scriptsize d \\\\).  Whenever \\\\(\\scriptsize d \\\\) is relatively small (e.g., less than a few thousand) we can quickly perform this eigendecomposition locally.\n",
    "\n",
    "Your task is to:\n",
    "1. Use a function from `numpy.linalg` called [eigh](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) to perform the eigendecomposition.  \n",
    "2. Next, sort the eigenvectors based on their corresponding eigenvalues (from high to low), yielding a matrix where the columns are the eigenvectors (and the first column is the top eigenvector).  Note that [np.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy-argsort) can be used to obtain the indices of the eigenvalues that correspond to the ascending order of eigenvalues.  \n",
    "3. Finally, set the `top_component` variable equal to the top eigenvector or prinicipal component, which is a \\\\(\\scriptsize 2 \\\\)-dimensional vector (array with two values).\n",
    "\n",
    "> Note:\n",
    "> * The eigenvectors returned by `eigh` appear in the columns and not the rows.\n",
    "> * For example, the first eigenvector of `eig_vecs` would be found in the first column and could be accessed using `eig_vecs[:,0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edafc3bf-55f6-4757-aa51-aac8433be349",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12877175dfb929b3ca701e326ea9f273",
     "grade": false,
     "grade_id": "answer_eigenDecomposition1d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to calculate eigenvalues and eigenvectors\n",
    "from numpy.linalg import eigh\n",
    "\n",
    "def compute_eigenvectors(covariance_matrix):\n",
    "    \"\"\"\n",
    "    TODO: \n",
    "            1) Calculate the eigenvalues and eigenvectors from correlated_cov_auto\n",
    "            2) Use np.argsort to determine the sorted indices based on the eigenvalues [in descending order]\n",
    "            3) Sort the eigenvalues and eigenvectors using the indices\n",
    "    \n",
    "    :param covariance_matrix: a np.ndarray covariance matrix\n",
    "    :return: (eigenvalues, eigenvectors)\n",
    "        WHERE\n",
    "            eigenvalues - a np.ndarray of eigenvalues sorted in descending order\n",
    "            eigenvectors - a np.ndarray of eigenvectors sorted by descending order of eigenvalues\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# eig_vals, eig_vecs = compute_eigenvectors(correlated_cov_auto)   \n",
    "# top_component = # <FILL IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('eigenvalues: {0}'.format(eig_vals))\n",
    "print('\\neigenvectors: \\n{0}'.format(eig_vecs))\n",
    "print('\\ntop principal component: {0}'.format(top_component))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adc386fc-def3-4b9d-8a27-b3ea851902b4",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07ea4e8493e89df3156a2768697f9d2d",
     "grade": true,
     "grade_id": "test_eigenDecomposition1d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Eigendecomposition (1d)\n",
    "def check_basis(vectors, correct):\n",
    "    return np.allclose(vectors, correct) or np.allclose(np.negative(vectors), correct)\n",
    "\n",
    "assert_true(check_basis(top_component, [0.68915649, 0.72461254]),\n",
    "                'incorrect value for top_component')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d4f93ce-f44c-4e74-af97-0ff1637fcb12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (1e) PCA scores\n",
    "\n",
    "We just computed the top principal component for a 2-dimensional non-spherical dataset.  Now let's use this principal component to derive a one-dimensional representation for the original data. To compute these compact representations, which are sometimes called PCA \"scores\", calculate the dot product between each data point in the raw data and the top principal component. In general, PCA \"scores\" are calculated by computing the dot product between the datapoint and each of the top k principal components for all the datapoints in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f3ddb8e-e2fa-4943-a297-47c2e9785a08",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "734aa86137cd5358cc982c87e38ecc75",
     "grade": false,
     "grade_id": "answer_PCAscores1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_pca_scores(dataset, top_k_eigenvectors):\n",
    "    \"\"\"\n",
    "    TODO: use the top_k_eigenvectors and the data from correlated_data to generate PCA scores\n",
    "    \n",
    "    :param dataset: data\n",
    "    :param top_k_eigenvectors: a np.ndarray of top k principal components\n",
    "    :return: (pca_scores)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "# Compute the 1 dimensional representation of the data using the top principal component\n",
    "correlated_data_scores = generate_pca_scores(correlated_data, np.array([top_component])).flatMap(lambda x: x)\n",
    "\n",
    "print('one-dimensional data (first three):\\n{0}'.format(np.asarray(correlated_data_scores.take(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78a401ef-2ecc-473f-b5b7-dcd8083b3d90",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b848a18f05187c1c6b296e5dd07b097a",
     "grade": true,
     "grade_id": "test_PCAscores1e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST PCA Scores (1e)\n",
    "first_three = [70.51682806, 69.30622356, 71.13588168]\n",
    "assert_true(check_basis(correlated_data_scores.take(3), first_three),\n",
    "                'incorrect value for correlated_data_scores')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12665dc6-e424-45fc-a396-5b52f5798554",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 2: Write a PCA function and evaluate PCA on sample datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "722375f0-698e-43bd-8651-fc504085b221",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (2a) PCA function\n",
    "\n",
    "We now have all the ingredients to write a general PCA function.  Instead of working with just the top principal component, our function will compute the top \\\\(\\scriptsize k\\\\) principal components and principal scores for a given dataset. The top \\\\(\\scriptsize k\\\\) principal components should be returned in descending order when ranked by their corresponding principal scores. Write this general function `pca`, and run it with `correlated_data` and \\\\(\\scriptsize k = 2\\\\). Hint: Use results from Part (1c), Part (1d), and Part (1e).\n",
    "\n",
    "Note: As discussed in lecture, our implementation is a reasonable strategy when \\\\(\\scriptsize d \\\\) is small, though more efficient distributed algorithms exist when \\\\(\\scriptsize d \\\\) is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d55e6c7c-baef-4bdb-8499-0ae7bb52bfbd",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e145df45ebc8445ae906d8bf90dce687",
     "grade": false,
     "grade_id": "answer_PCAfunction2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to compute top-k principle components, scores and all eigenvalues\n",
    "\n",
    "def pca(data, k=2):\n",
    "    \"\"\"\n",
    "    TODO: Computes the top 'k' principal components, corresponding scores, and ALL EIGENVALUES.\n",
    "            1) compute the covariance matrix\n",
    "            2) obtain the eigenvectors and eigenvalues\n",
    "            3) compute the PCA scores using the top 'k' eigenvectors\n",
    "            4) return the 'k' principal components, 'k' scores, and all eigenvalues\n",
    "        HINT: Utilize functions implemented in 1c, 1d, 1e. \n",
    "    \n",
    "    NOTE:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). This \n",
    "        function should also return eigenvectors as columns.\n",
    " \n",
    "    :param data: an 'RDD' consisting of NumPy arrays\n",
    "    :param k: the number of principal components to return (int)\n",
    "\n",
    "    :return: (top-k principle components, 'RDD' of scores, sorted eigenvalues)\n",
    "        WHERE\n",
    "            top-k principle components - a np.ndarray where the 'k' columns correspond to the\n",
    "                            'k' top principal components\n",
    "            'RDD' of scores - a RDD of np.ndarray where the number of rows is the same as the \n",
    "                            input 'RDD' and consists of arrays of length 'k'\n",
    "            sorted eigenvalues - a sorted np.ndarray of length d (the number of features)\n",
    "    \"\"\"\n",
    "    # sample_cov = <FILL_IN>\n",
    "    # eig_vals, eig_vecs = <FILL_IN>\n",
    "    # eig_vals_indices = <FILL_IN>\n",
    "    # # Return the `k` principal components, `k` scores, and all eigenvalues\n",
    "    # components = <FILL_IN>\n",
    "    # eigenvalues = <FILL_IN\n",
    "    # scores = data.map(<FILL_IN>)\n",
    "    # return <FILL_IN>\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Run pca on correlated_data with k = 2\n",
    "top_components_correlated, correlated_data_scores_auto, eigenvalues_correlated  = pca(correlated_data, 2)\n",
    "\n",
    "# Note that the 1st principal component is in the first column\n",
    "print('top_components_correlated: \\n{0}'.format(top_components_correlated))\n",
    "print('\\ncorrelated_data_scores_auto (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, correlated_data_scores_auto.take(3)))))\n",
    "print('\\neigenvalues_correlated: \\n{0}'.format(eigenvalues_correlated))\n",
    "\n",
    "# Create a higher dimensional test set\n",
    "pca_test_data = sc.parallelize([np.arange(x, x + 4) for x in np.arange(0, 20, 4)])\n",
    "components_test, test_scores, eigenvalues_test = pca(pca_test_data, 3)\n",
    "\n",
    "print('\\npca_test_data: \\n{0}'.format(np.array(pca_test_data.collect())))\n",
    "print('\\ncomponents_test: \\n{0}'.format(components_test))\n",
    "print('\\ntest_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, test_scores.take(3)))))\n",
    "print('\\neigenvalues_test: \\n{0}'.format(eigenvalues_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0ada58a-fcdc-41b5-aed0-9db8096ed13c",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b17c487770f2fa9b75db38034a426d76",
     "grade": true,
     "grade_id": "test_PCAfunction2a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST PCA Function (2a)\n",
    "assert_true(check_basis(top_components_correlated.T,\n",
    "                            [[0.68915649,  0.72461254], [-0.72461254, 0.68915649]]),\n",
    "                'incorrect value for top_components_correlated')\n",
    "first_three_correlated = [[70.51682806, 69.30622356, 71.13588168], [1.48305648, 1.5888655, 1.86710679]]\n",
    "assert_true(np.allclose(first_three_correlated,\n",
    "                            np.vstack(np.abs(correlated_data_scores_auto.take(3))).T, atol=1e-2),\n",
    "                'incorrect value for first three correlated values')\n",
    "assert_true(np.allclose(eigenvalues_correlated, [1.94345403, 0.13820481], atol=1e-2),\n",
    "                           'incorrect values for eigenvalues_correlated')\n",
    "top_components_correlated_k1, correlated_data_scores_k1, eigenvalues_correlated_k1 = pca(correlated_data, 1)\n",
    "assert_true(check_basis(top_components_correlated_k1.T, [0.68915649, 0.72461254]),\n",
    "               'incorrect value for components when k=1')\n",
    "assert_true(np.allclose([70.51682806, 69.30622356, 71.13588168],\n",
    "                            np.vstack(np.abs(correlated_data_scores_k1.take(3))).T, atol=1e-2),\n",
    "                'incorrect value for scores when k=1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "564df88f-68ec-47ca-a9a3-76d23c3f722a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (2b) PCA on `data_random`\n",
    "\n",
    "Next, use the PCA function we just developed to find the top two principal components of the spherical `data_random` we created in Visualization 1.\n",
    "\n",
    "First, we need to convert `data_random` to the RDD `random_data_rdd`, and do all subsequent operations on `random_data_rdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff496f50-022e-40af-a6fb-b1b44e678b2b",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b0b13681104a1274e92fced37fc652a",
     "grade": false,
     "grade_id": "answer_PCADataRandom2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to convert data_random to RDD random_data_rdd\n",
    "random_data_rdd = sc.parallelize(data_random)\n",
    "\n",
    "# # Use pca on data_random\n",
    "# top_components_random, random_data_scores_auto, eigenvalues_random = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('top_components_random: \\n{0}'.format(top_components_random))\n",
    "print('\\nrandom_data_scores_auto (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, random_data_scores_auto.take(3)))))\n",
    "print('\\neigenvalues_random: \\n{0}'.format(eigenvalues_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89ab732e-923f-47d1-82f1-47541effb7be",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74d0e39381f7f8160b1bccd803449054",
     "grade": true,
     "grade_id": "test_PCADataRandom2b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST PCA on `data_random` (2b)\n",
    "assert_true(check_basis(top_components_random.T,\n",
    "                            [[-0.2522559 ,  0.96766056], [-0.96766056,  -0.2522559]]),\n",
    "                'incorrect value for top_components_random')\n",
    "first_three_random = [[36.61068572, 35.97314295, 35.59836628],\n",
    "                      [61.3489929 ,  62.08813671,  60.61390415]]\n",
    "assert_true(np.allclose(first_three_random, np.vstack(np.abs(random_data_scores_auto.take(3))).T, atol=1e-2),\n",
    "                'incorrect value for random_data_scores_auto')\n",
    "assert_true(np.allclose(eigenvalues_random, [1.4204546, 0.99521397], atol=1e-2),\n",
    "                            'incorrect value for eigenvalues_random')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d787cf6e-0236-4deb-8034-6b99042ef66d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 2: PCA projection\n",
    "\n",
    "Plot the original data and the 1-dimensional reconstruction using the top principal component to see how the PCA solution looks.  The original data is plotted as before; however, the 1-dimensional reconstruction (projection) is plotted in green on top of the original data and the vectors (lines) representing the two principal components are shown as dotted lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "400ef79d-dc7f-4ccc-b3dd-dccc91613509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def project_points_and_get_lines(data, components, x_range):\n",
    "    \"\"\"Project original data onto first component and get line details for top two components.\"\"\"\n",
    "    top_component = components[:, 0]\n",
    "    slope1, slope2 = components[1, :2] / components[0, :2]\n",
    "\n",
    "    means = data.mean()[:2]\n",
    "    demeaned = data.map(lambda v: v - means)\n",
    "    projected = demeaned.map(lambda v: (v.dot(top_component) /\n",
    "                                        top_component.dot(top_component)) * top_component)\n",
    "    remeaned = projected.map(lambda v: v + means)\n",
    "    x1,x2 = zip(*remeaned.collect())\n",
    "\n",
    "    line_start_P1_X1, line_start_P1_X2 = means - np.asarray([x_range, x_range * slope1])\n",
    "    line_end_P1_X1, line_end_P1_X2 = means + np.asarray([x_range, x_range * slope1])\n",
    "    line_start_P2_X1, line_start_P2_X2 = means - np.asarray([x_range, x_range * slope2])\n",
    "    line_end_P2_X1, line_end_P2_X2 = means + np.asarray([x_range, x_range * slope2])\n",
    "\n",
    "    return ((x1, x2), ([line_start_P1_X1, line_end_P1_X1], [line_start_P1_X2, line_end_P1_X2]),\n",
    "            ([line_start_P2_X1, line_end_P2_X1], [line_start_P2_X2, line_end_P2_X2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f54be9f7-53c6-4763-b1d0-e6fd4dba9734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
    "    project_points_and_get_lines(correlated_data, top_components_correlated, 5)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
    "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
    "plt.scatter(data_correlated[:,0], data_correlated[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "605bb089-8956-499c-9cf2-13d0660c6189",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
    "    project_points_and_get_lines(random_data_rdd, top_components_random, 5)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
    "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
    "plt.scatter(data_random[:,0], data_random[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7405a52-b59b-4f22-beb5-37fe7ca1fc47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 3: Three-dimensional data\n",
    "\n",
    "So far we have worked with two-dimensional data. Now let's generate three-dimensional data with highly correlated features. As in Visualization 1, we'll create samples from a multivariate Gaussian distribution, which in three dimensions requires us to specify three means, three variances, and three covariances.\n",
    "\n",
    "In the 3D graphs below, we have included the 2D plane that corresponds to the top two principal components, i.e. the plane with the smallest euclidean distance between the points and itself. Notice that the data points, despite living in three-dimensions, are found near a two-dimensional plane: the left graph shows how most points are close to the plane when it is viewed from its side, while the right graph shows that the plane covers most of the variance in the data.  Note that darker blues correspond to points with higher values for the third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f4ef6c8-1e67-4ed2-9781-b94e094a53fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "m = 100\n",
    "mu = np.array([50, 50, 50])\n",
    "r1_2 = 0.9\n",
    "r1_3 = 0.7\n",
    "r2_3 = 0.1\n",
    "sigma1 = 5\n",
    "sigma2 = 20\n",
    "sigma3 = 20\n",
    "c = np.array([[sigma1 ** 2, r1_2 * sigma1 * sigma2, r1_3 * sigma1 * sigma3],\n",
    "             [r1_2 * sigma1 * sigma2, sigma2 ** 2, r2_3 * sigma2 * sigma3],\n",
    "             [r1_3 * sigma1 * sigma3, r2_3 * sigma2 * sigma3, sigma3 ** 2]])\n",
    "np.random.seed(142)\n",
    "data_threeD = np.random.multivariate_normal(mu, c, m)\n",
    "\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from matplotlib.cm import get_cmap\n",
    "norm = Normalize()\n",
    "cmap = get_cmap(\"Blues\")\n",
    "clrs = cmap(np.array(norm(data_threeD[:,2])))[:,0:3]\n",
    "\n",
    "fig = plt.figure(figsize=(11, 6))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.azim=-100\n",
    "ax.scatter(data_threeD[:,0], data_threeD[:,1], data_threeD[:,2], c=clrs, s=14**2)\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(-15, 10, 1), np.arange(-50, 30, 1))\n",
    "normal = np.array([0.96981815, -0.188338, -0.15485978])\n",
    "z = (-normal[0] * xx - normal[1] * yy) * 1. / normal[2]\n",
    "xx = xx + 50\n",
    "yy = yy + 50\n",
    "z = z + 50\n",
    "\n",
    "ax.set_zlim((-20, 120)), ax.set_ylim((-20, 100)), ax.set_xlim((30, 75))\n",
    "ax.plot_surface(xx, yy, z, alpha=.10)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.azim=10\n",
    "ax.elev=20\n",
    "#ax.dist=8\n",
    "ax.scatter(data_threeD[:,0], data_threeD[:,1], data_threeD[:,2], c=clrs, s=14**2)\n",
    "\n",
    "ax.set_zlim((-20, 120)), ax.set_ylim((-20, 100)), ax.set_xlim((30, 75))\n",
    "ax.plot_surface(xx, yy, z, alpha=.1)\n",
    "plt.tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "037a7bb2-7424-48fc-b93e-61bb22289e49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (2c) 3D to 2D\n",
    "\n",
    "We will now use PCA to see if we can recover the 2-dimensional plane on which the data live. Parallelize the data, and use our PCA function from above, with \\\\( \\scriptsize k=2 \\\\) components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b37a4395-38be-4bf3-a653-bbc71dcc8124",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e25edf63127ddd2ff2a54cd97e1b356",
     "grade": false,
     "grade_id": "answer_PCAThreeDimension2c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to apply PCA function to 3D data. You need to first parallelize the data and use the PCA function defined above with k=2 components\n",
    "threeD_data = sc.parallelize(data_threeD)\n",
    "\n",
    "# components_threeD, threeD_scores, eigenvalues_threeD = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('components_threeD: \\n{0}'.format(components_threeD))\n",
    "print('\\nthreeD_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, threeD_scores.take(3)))))\n",
    "print('\\neigenvalues_threeD: \\n{0}'.format(eigenvalues_threeD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7f49af6-da53-44b1-98e7-a067d71c3641",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a26131a02ae5984bdb1537244c243ea7",
     "grade": true,
     "grade_id": "test_PCAThreeDimension2c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST 3D to 2D (2c)\n",
    "assert_equal(components_threeD.shape, (3, 2), 'incorrect shape for components_threeD')\n",
    "assert_true(np.allclose(np.sum(eigenvalues_threeD), 969.796443367, atol=1e-2),\n",
    "                'incorrect value for eigenvalues_threeD')\n",
    "assert_true(np.allclose(np.abs(np.sum(components_threeD)), 1.77238943258, atol=1e-2),\n",
    "                'incorrect value for components_threeD')\n",
    "assert_true(np.allclose(np.abs(np.sum(threeD_scores.take(3))), 237.782834092, atol=1e-2),\n",
    "                'incorrect value for threeD_scores')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afe76000-82c9-4d56-96de-2057a7cb3f91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 4: 2D representation of 3D data\n",
    "\n",
    "See the 2D version of the data that captures most of its original structure.  Note that darker blues correspond to points with higher values for the original data's third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "904ebbee-ece7-400f-8ee1-1783ad71e369",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scores_threeD = np.asarray(threeD_scores.collect())\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(20, 150, 20), np.arange(-40, 110, 20))\n",
    "ax.set_xlabel(r'New $x_1$ values'), ax.set_ylabel(r'New $x_2$ values')\n",
    "ax.set_xlim(5, 150), ax.set_ylim(-45, 50)\n",
    "plt.scatter(scores_threeD[:, 0], scores_threeD[:, 1], s=14 ** 2, c=clrs, edgecolors='#8cbfd0', alpha=0.75)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1fff30d-17c1-4823-85c2-dd84367c94df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (2d) Variance explained\n",
    "\n",
    "Finally, let's quantify how much of the variance is being captured by PCA in each of the three synthetic datasets we've analyzed.  To do this, we'll compute the fraction of retained variance by the top principal components.  Recall that the eigenvalue corresponding to each principal component captures the variance along this direction.  If our initial data is \\\\(\\scriptsize d\\\\)-dimensional, then the total variance in our data equals: \\\\( \\scriptsize \\sum_{i=1}^d \\lambda_i \\\\), where \\\\(\\scriptsize \\lambda_i\\\\) is the eigenvalue corresponding to the \\\\(\\scriptsize i\\\\)th principal component. Moreover, if we use PCA with some \\\\(\\scriptsize k < d\\\\), then we can compute the variance retained by these principal components by adding the top \\\\(\\scriptsize k\\\\) eigenvalues.  The fraction of retained variance equals the sum of the top \\\\(\\scriptsize k\\\\) eigenvalues divided by the sum of all of the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e22ca878-4d41-4656-b9de-ef696aac589a",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23f349ee189f895f99776c5e5062bd4f",
     "grade": false,
     "grade_id": "answer_varianceExplained2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to compute the fraction of retained variance by the top principle components\n",
    "\n",
    "def variance_explained(data, k=1):\n",
    "    \"\"\"\n",
    "    TODO: Calculate the fraction of variance explained by the top `k` eigenvectors.\n",
    "            1) compute the eigenvalues (HINT: utilize previously implemented function)\n",
    "            2) compute the variance\n",
    "\n",
    "    :param data: an RDD of np.ndarray which stores the features for an observation\n",
    "    :param k: the number of principal components to consider\n",
    "\n",
    "    :return: (retained variance)\n",
    "        WHERE\n",
    "            retained variance - a number between 0 and 1 representing the percentage of \n",
    "                                variance explained by the top `k` eigenvectors\n",
    "    \"\"\"\n",
    "    # components, scores, eigenvalues = <FILL_IN>\n",
    "    # variances = <FILL_IN>\n",
    "    # return <FILL_IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "variance_random_1 = variance_explained(random_data_rdd, 1)\n",
    "variance_correlated_1 = variance_explained(correlated_data, 1)\n",
    "variance_random_2 = variance_explained(random_data_rdd, 2)\n",
    "variance_correlated_2 = variance_explained(correlated_data, 2)\n",
    "variance_threeD_2 = variance_explained(threeD_data, 2)\n",
    "print ('Percentage of variance explained by the first component of random_data_rdd: {0:.1f}%'\n",
    "       .format(variance_random_1 * 100))\n",
    "print ('Percentage of variance explained by both components of random_data_rdd: {0:.1f}%'\n",
    "       .format(variance_random_2 * 100))\n",
    "print ('\\nPercentage of variance explained by the first component of correlated_data: {0:.1f}%'.\n",
    "       format(variance_correlated_1 * 100))\n",
    "print ('Percentage of variance explained by both components of correlated_data: {0:.1f}%'\n",
    "       .format(variance_correlated_2 * 100))\n",
    "print ('\\nPercentage of variance explained by the first two components of threeD_data: {0:.1f}%'\n",
    "       .format(variance_threeD_2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "350e400c-53c0-455e-b6ac-1e80c118c3e7",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "623c85dd43aa5ff2d855cf0152340d91",
     "grade": true,
     "grade_id": "test_varianceExplained2d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Variance explained (2d)\n",
    "assert_true(np.allclose(variance_random_1, 0.588017172066, atol=1e-2), 'incorrect value for variance_random_1')\n",
    "assert_true(np.allclose(variance_correlated_1, 0.933608329586, atol=1e-2),\n",
    "                'incorrect value for varianceCorrelated1')\n",
    "assert_true(np.allclose(variance_random_2, 1.0, atol=1e-2), 'incorrect value for variance_random_2')\n",
    "assert_true(np.allclose(variance_correlated_2, 1.0, atol=1e-2), 'incorrect value for variance_correlated_2')\n",
    "assert_true(np.allclose(variance_threeD_2, 0.993967356912, atol=1e-2), 'incorrect value for variance_threeD_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132a5dfa-cbad-4782-8690-1bd474701a48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 3:  Parse, inspect, and preprocess neuroscience data then perform PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "277a9697-66eb-4502-aff4-b3bb4866bcc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data introduction\n",
    "\n",
    "A central challenge in neuroscience is understanding the organization and function of neurons, the cells responsible for processing and representing information in the brain. New technologies make it possible to monitor the responses of large populations of neurons in awake animals. In general, neurons communicate through electrical impulses that must be recorded with electrodes, which is a challenging process. As an alternative, we can genetically engineer animals so that their neurons express special proteins that fluoresce or light up when active, and then use microscopy to record neural activity as images.\n",
    "\n",
    "Light-sheet microscopy lets us do this in a special, transparent animal, the larval zebrafish, over nearly its entire brain. The resulting data are time-varying images containing the activity of hundreds of thousands of neurons. Given the raw data, which is enormous, we want to find compact spatial and temporal patterns: Which groups of neurons are active together? What is the time course of their activity? Are those patterns specific to particular events happening during the experiment (e.g. a stimulus that we might present). PCA is a powerful technique for finding spatial and temporal patterns in these kinds of data, and that's what we'll explore here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55869c87-c26a-4b3d-842f-b1750035030e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (3a) Load neuroscience data\n",
    "\n",
    "In the next sections we will use PCA to capture structure in neural datasets. Before doing the analysis, we will load and do some basic inspection of the data. The raw data are currently stored as a text file. Every line in the file contains the time series of image intensity for a single pixel in a time-varying image (i.e. a movie). The first two numbers in each line are the spatial coordinates of the pixel, and the remaining numbers are the time series. We'll use `first()` to inspect a single row, and print just the first 100 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b3be267-20be-46ae-800b-b4d0e1e5dc40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/10605/data/master/hw3/neuro.txt'\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "sc.addFile(url)\n",
    "\n",
    "lines = sc.textFile(\"file://\" + SparkFiles.get(\"neuro.txt\"))\n",
    "print(lines.first()[0:100])\n",
    "\n",
    "# Check that everything loaded properly\n",
    "assert len(lines.first()) == 1397\n",
    "assert lines.count() == 46460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f02508e5-bd1e-43d5-b802-b7559bcf8deb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (3b) Parse the data\n",
    "\n",
    "Parse the data into a key-value representation. We want each key to be a tuple of two-dimensional spatial coordinates and each value to be a NumPy array storing the associated time series. Write a function that converts a line of text into a (`tuple`, `np.ndarray`) pair. Then apply this function to each record in the RDD, and inspect the first entry of the new parsed data set. Now would be a good time to cache the data, and force a computation by calling count, to ensure the data are cached.\n",
    "\n",
    "Note: `tuple` contains 2 integer values and the numpy array consists of float numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "236ee1e2-f55c-4155-a1a9-dac7146fece1",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a409ce48ee547513dcfddc654d251511",
     "grade": false,
     "grade_id": "answer_parseData3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to parse the data into (`tuple`, `np.ndarray`) format \n",
    "\n",
    "def parse(line):\n",
    "    \"\"\"\n",
    "    TODO: Parse the raw data into a (`tuple`, `np.ndarray`) pair.\n",
    "            1) Split the line\n",
    "            2) obtain the coordinate and the pixel intensity array \n",
    "                (HINT: read the description about the parameter)\n",
    "\n",
    "    NOTE:\n",
    "        You should store the pixel coordinates as a tuple of two ints and the elements of the \n",
    "        pixel intensity time series as an np.ndarray of floats.\n",
    "\n",
    "    :param line: a string representing an observation. Elements are separated by spaces. The\n",
    "            first two elements represent the coordinates of the pixel, and the rest of the elements\n",
    "            represent the pixel intensity over time.\n",
    "\n",
    "    :return: (coordinate, pixel intensity array)\n",
    "        WHERE\n",
    "            coordinate - a tuple containing two values \n",
    "            pixel intensity array - the pixel intensity is stored in an np.ndarray which contains \n",
    "                                    240 values\n",
    "    \"\"\"\n",
    "    # vector_split = <FILL_IN>\n",
    "    # pixel_coords = <FILL_IN>\n",
    "    # pixel_intensity = <FILL_IN>\n",
    "    # return <FILL_IN>\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "raw_data = lines.map(parse)\n",
    "raw_data.cache()\n",
    "entry = raw_data.first()\n",
    "print ('Length of movie is {0} seconds'.format(len(entry[1])))\n",
    "print ('Number of pixels in movie is {0:,}'.format(raw_data.count()))\n",
    "print ('\\nFirst entry of raw_data (with only the first five values of the NumPy array):\\n({0}, {1})'\n",
    "       .format(entry[0], entry[1][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68b13145-5e8a-4232-815e-d54fd057add8",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cd075abdac594cec83c9bb7cef5f38c",
     "grade": true,
     "grade_id": "test_parseData3b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Parse the data (3b)\n",
    "assert_true(isinstance(entry[0], tuple), \"entry's key should be a tuple\")\n",
    "assert_equal(len(entry), 2, 'entry should have a key and a value')\n",
    "assert_true(isinstance(entry[0][1], int), 'coordinate tuple should contain ints')\n",
    "assert_equal(len(entry[0]), 2, \"entry's key should have two values\")\n",
    "assert_true(isinstance(entry[1], np.ndarray), \"entry's value should be an np.ndarray\")\n",
    "assert_true(isinstance(entry[1][0], np.float64), 'the np.ndarray should consist of np.float values')\n",
    "assert_equal(entry[0], (0, 0), 'incorrect key for entry')\n",
    "assert_equal(entry[1].size, 240, 'incorrect length of entry array')\n",
    "assert_true(np.allclose(np.sum(entry[1]), 24683.5), 'incorrect values in entry array')\n",
    "assert_true(raw_data.is_cached, 'raw_data is not cached')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "965b7d2a-ad2b-46ae-9f08-9568949fa5bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (3c) Min and max fluorescence\n",
    "\n",
    "Next we'll do some basic preprocessing on the data. The raw time-series data are in units of image fluorescence, and baseline fluorescence varies somewhat arbitrarily from pixel to pixel. First, compute the minimum and maximum values across all pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "308f6198-2422-4121-88ee-612700a4ec9d",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55c19b748633432692f8ab2dd4aa77c2",
     "grade": false,
     "grade_id": "answer_MinMax3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to calculate min and max\n",
    "# mn = raw_data.<FILL_IN>\n",
    "# mx = raw_data.<FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print (mn, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f28b68f0-042d-45ff-a7e3-936f92e5ef27",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01e87aab41f955f75da7cb14313369d8",
     "grade": true,
     "grade_id": "test_MinMax3c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Min and max fluorescence (3c)\n",
    "assert_true(np.allclose(mn, 100.6, atol=1e-2), 'incorrect value for mn')\n",
    "assert_true(np.allclose(mx, 940.8, atol=1e-2), 'incorrect value for mx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab07fb0c-8ed9-4393-b7ef-0796fa338f51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 5: Pixel intensity\n",
    "\n",
    "Let's now see how a random pixel varies in value over the course of the time series.  We'll visualize a pixel that exhibits a standard deviation of over 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3627c551-319d-4e4e-a434-99c9b70565e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example = raw_data.filter(lambda x: np.std(x[1]) > 100).values().first()\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 300, 50), np.arange(300, 800, 100))\n",
    "ax.set_xlabel(r'time'), ax.set_ylabel(r'fluorescence')\n",
    "ax.set_xlim(-20, 270), ax.set_ylim(270, 730)\n",
    "plt.plot(range(len(example)), example, c='#8cbfd0', linewidth='3.0')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a80f4ce2-0c14-496c-9e48-9c170f952666",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (3d) Fractional signal change\n",
    "\n",
    "To convert from these raw fluorescence units to more intuitive units of fractional signal change, write a function that takes a time series for a particular pixel and subtracts and divides by the mean.  Then apply this function to all the pixels. Confirm that this changes the maximum and minimum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72c6c57d-2518-4d0c-b9ca-9be8db6bbca6",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79de3f9d7d95736be200f710cc521e6d",
     "grade": false,
     "grade_id": "answer_rescale3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to standardize the array\n",
    "\n",
    "def rescale(ts):\n",
    "    \"\"\"\n",
    "    TODO: take a np.ndarray and return the standardized array by subtracting and dividing by the mean\n",
    "\n",
    "    NOTE:\n",
    "        You should first subtract the mean and then divide by the mean.\n",
    "\n",
    "    :param ts: an np.ndarray of time series data representing pixel intensity.\n",
    "\n",
    "    :return: (standardized array)\n",
    "        WHERE\n",
    "           standardized array - a an np.ndarray of the times series adjusted by \n",
    "                                subtracting the mean and dividing by the mean.\n",
    "    \"\"\"\n",
    "    # return <FILL_IN>\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "scaled_data = raw_data.mapValues(lambda v: rescale(v))\n",
    "mn_scaled = scaled_data.map(lambda x: x[1]).map(lambda v: min(v)).min()\n",
    "mx_scaled = scaled_data.map(lambda x: x[1]).map(lambda v: max(v)).max()\n",
    "print(mn_scaled, mx_scaled)\n",
    "\n",
    "scaled_data_item = scaled_data.first()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78a813dd-28cf-4c75-b228-8b0dd44256ba",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fda17f6734e0a761f7ab41ba0d8f7a42",
     "grade": true,
     "grade_id": "test_rescale3d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Fractional signal change (3d)\n",
    "assert_true(isinstance(scaled_data_item, np.ndarray), 'incorrect type returned by rescale')\n",
    "assert_true(np.allclose(mn_scaled, -0.27151288, atol=1e-2), 'incorrect value for mn_scaled')\n",
    "assert_true(np.allclose(mx_scaled, 0.90544876, atol=1e-2), 'incorrect value for mx_scaled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b498d013-0808-40b6-a96c-500df1b50d49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 6: Normalized data\n",
    "\n",
    "Now that we've normalized our data, let's once again see how a random pixel varies in value over the course of the time series.  We'll visualize a pixel that exhibits a standard deviation of over 0.1.  Note the change in scale on the y-axis compared to the previous visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2097eee9-43b1-4ac4-9d20-b552f5f72c21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example = scaled_data.filter(lambda x: np.std(x[1]) > 0.1).values().first()\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 300, 50), np.arange(-.1, .6, .1))\n",
    "ax.set_xlabel(r'time'), ax.set_ylabel(r'fluorescence')\n",
    "ax.set_xlim(-20, 260), ax.set_ylim(-.12, .52)\n",
    "plt.plot(range(len(example)), example, c='#8cbfd0', linewidth='3.0')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e6c451-05d4-4c32-9ec7-1b9607da3131",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (3e) PCA on the scaled data\n",
    "\n",
    "We now have a preprocessed dataset with \\\\(\\scriptsize n = 46460\\\\) pixels and \\\\(\\scriptsize d = 240\\\\) seconds of time series data for each pixel.  We can interpret the pixels as our observations and each pixel value in the time series as a feature.  We would like to find patterns in brain activity during this time series, and we expect to find correlations over time.  We can thus use PCA to find a more compact representation of our data and allow us to visualize it.\n",
    "\n",
    "Use the `pca` function from Part (2a) to perform PCA on the preprocessed neuroscience data with \\\\(\\scriptsize k = 3\\\\), resulting in a new low-dimensional 46460 by 3 dataset.  The `pca` function takes an RDD of arrays, but `scaled_data` is an RDD of key-value pairs, so you'll need to extract the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e93d0e9-24c1-4e9b-9fca-4a8dd79d845a",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c81af4c0f3e3702d825b4d064cb4be25",
     "grade": false,
     "grade_id": "answer_PCAscaled3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to run pca using scaled_data\n",
    "# scaled_data_extracted is the rdd where only the values of scaled_data is extracted. \n",
    "# PCA function takes an RDD of arrays, so we need to extract the values from scaled_data (RDD of key-value pairs)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# scaled_data_extracted = scaled_data.<FILL_IN>\n",
    "# components_scaled, scaled_scores, eigenvalues_scaled = <FILL_IN>\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print ('components_scaled: (first five) \\n{0}'.format(components_scaled[:5, :]))\n",
    "print ('\\nscaled_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, scaled_scores.take(3)))))\n",
    "print ('\\neigenvalues_scaled: (first five) \\n{0}'.format(eigenvalues_scaled[:5]))\n",
    "\n",
    "components_scaled_shape = components_scaled.shape\n",
    "components_scaled_sum = np.abs(np.sum(components_scaled[:5, :]))\n",
    "scaled_scores_sum = np.abs(np.sum(scaled_scores.take(3)))\n",
    "eigenvalues_scaled_sum = np.sum(eigenvalues_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc2cf3e1-fc5a-4794-a723-d997a8675e4e",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3db802d2290886a1c5fe58dc8b8b4985",
     "grade": true,
     "grade_id": "test_PCAscaled3e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST PCA on the scaled data (3e)\n",
    "assert_equal(components_scaled_shape, (240, 3), 'incorrect shape for components_scaled')\n",
    "assert_true(np.allclose(components_scaled_sum, 0.283150995232, atol=1e-2),\n",
    "                'incorrect value for components_scaled')\n",
    "assert_true(np.allclose(scaled_scores_sum, 0.0285507449251, atol=1e-2),\n",
    "                'incorrect value for scaled_scores')\n",
    "assert_true(np.allclose(eigenvalues_scaled_sum, 0.206987501564, atol=1e-2),\n",
    "                'incorrect value for eigenvalues_scaled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f86f03b0-b341-4e4a-b854-c3a70802f63a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 7: Top two components as images\n",
    "\n",
    "Now, we'll view the scores for the top two components as images.  Note that we reshape the vectors by the dimensions of the original image, 230 x 202.\n",
    "These graphs map the values for the single component to a grayscale image.  This provides us with a visual representation which we can use to see the overall structure of the zebrafish brain and to identify where high and low values occur.  However, using this representation, there is a substantial amount of useful information that is difficult to interpret.  In the next visualization, we'll see how we can improve interpretability by combining the two principal components into a single image using a color mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fcd0fdf-a96a-4321-9383-fd13c5934caf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "scores_scaled = np.vstack(scaled_scores.collect())\n",
    "image_one_scaled = scores_scaled[:, 0].reshape(230, 202).T\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "ax.set_title('Top Principal Component', color='#888888')\n",
    "image = plt.imshow(image_one_scaled, interpolation='nearest', aspect='auto', cmap=cm.gray)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13eb489-076a-4438-b0bd-4772b93de0b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_two_scaled = scores_scaled[:, 1].reshape(230, 202).T\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "ax.set_title('Second Principal Component', color='#888888')\n",
    "image = plt.imshow(image_two_scaled, interpolation='nearest', aspect='auto', cmap=cm.gray)\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79f5a3e8-b1b4-40f2-a762-3746b42e8daf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 8: Top two components as one image\n",
    "\n",
    "When we perform PCA and color neurons based on their location in the low-dimensional space, we can interpret areas with similar colors as exhibiting similar responses (at least in terms of the simple representation we recover with PCA). Below, the first graph shows how low-dimensional representations, which correspond to the first two principal components, are mapped to colors. The second graph shows the result of this color mapping using the zebrafish neural data.\n",
    "\n",
    "The second graph clearly exhibits patterns of neural similarity throughout different regions of the brain.  However, when performing PCA on the full dataset, there are multiple reasons why neurons might have similar responses. The neurons might respond similarly to different stimulus directions, their responses might have  similar temporal dynamics, or their response similarity could be influenced by both temporal and stimulus-specific factors. However, with our initial PCA analysis, we cannot pin down the underlying factors, and hence it is hard to interpret what \"similarity\" really means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d5c45d7-cff0-4d11-9221-db69579b6d39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Optional Details: Note that we use [polar coordinates](https://en.wikipedia.org/wiki/Polar_coordinate_system) to map our low-dimensional points to colors.  Using polar coordinates provides us with an angle \\\\( (\\phi) \\\\) and magnitude \\\\( (\\rho) \\\\).  We then use the well-known polar color space, [hue-saturation-value](https://en.wikipedia.org/wiki/HSL_and_HSV) (HSV), and map the angle to hue and the magnitude to value (brightness).  This maps low magnitude points to black while allowing larger magnitude points to be differentiated by their angle. Additionally, the function `polarTransform` that maps low-dimensional representations to colors has an input parameter called `scale`, which we set  to 2.0, and you can try lower values for the two graphs to see more nuanced mappings -- values near 1.0 are particularly interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e34888a6-73a6-4344-9821-1d5f63c325be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from python-thunder's Colorize.transform where cmap='polar'.\n",
    "# Checkout the library at: https://github.com/thunder-project/thunder and\n",
    "# http://thunder-project.org/\n",
    "\n",
    "def polar_transform(scale, img):\n",
    "    \"\"\"Convert points from cartesian to polar coordinates and map to colors.\"\"\"\n",
    "    from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "    img = np.asarray(img)\n",
    "    dims = img.shape\n",
    "\n",
    "    phi = ((np.arctan2(-img[0], -img[1]) + np.pi/2) % (np.pi*2)) / (2 * np.pi)\n",
    "    rho = np.sqrt(img[0]**2 + img[1]**2)\n",
    "    saturation = np.ones((dims[1], dims[2]))\n",
    "\n",
    "    out = hsv_to_rgb(np.dstack((phi, saturation, scale * rho)))\n",
    "\n",
    "    return np.clip(out * scale, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26a48468-7021-42f5-b09e-61e7998abeae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the polar mapping from principal component coordinates to colors.\n",
    "x1_abs_max = np.max(np.abs(image_one_scaled))\n",
    "x2_abs_max = np.max(np.abs(image_two_scaled))\n",
    "\n",
    "num_of_pixels = 300\n",
    "x1_vals = np.arange(-x1_abs_max, x1_abs_max, (2 * x1_abs_max) / num_of_pixels)\n",
    "x2_vals = np.arange(x2_abs_max, -x2_abs_max, -(2 * x2_abs_max) / num_of_pixels)\n",
    "x2_vals.shape = (num_of_pixels, 1)\n",
    "\n",
    "x1_data = np.tile(x1_vals, (num_of_pixels, 1))\n",
    "x2_data = np.tile(x2_vals, (1, num_of_pixels))\n",
    "\n",
    "# Try changing the first parameter to lower values\n",
    "polar_map = polar_transform(2.0, [x1_data, x2_data])\n",
    "\n",
    "grid_range = np.arange(0, num_of_pixels + 25, 25)\n",
    "fig, ax = prepare_plot(grid_range, grid_range, figsize=(9.0, 7.2), hide_labels=True)\n",
    "image = plt.imshow(polar_map, interpolation='nearest', aspect='auto')\n",
    "ax.set_xlabel('Principal component one'), ax.set_ylabel('Principal component two')\n",
    "grid_marks = (2 * grid_range / float(num_of_pixels) - 1.0)\n",
    "x1_marks = x1_abs_max * grid_marks\n",
    "x2_marks = -x2_abs_max * grid_marks\n",
    "ax.get_xaxis().set_ticklabels(map(lambda x: '{0:.1f}'.format(x), x1_marks))\n",
    "ax.get_yaxis().set_ticklabels(map(lambda x: '{0:.1f}'.format(x), x2_marks))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb64f92a-2be8-4831-b4c3-79413e3a1241",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the same transformation on the image data\n",
    "# Try changing the first parameter to lower values\n",
    "brainmap = polar_transform(2.0, [image_one_scaled, image_two_scaled])\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap,interpolation='nearest', aspect='auto')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb61140d-5bb1-45c2-aa84-6b029ebc3e94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 4: Feature-based aggregation and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "245ce1fa-d7ea-4d0d-be98-b957e2057087",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (4a) Aggregation using arrays\n",
    "\n",
    "In the analysis in Part 3, we performed PCA on the full time series data, trying to find global patterns across all 240 seconds of the time series. However, our analysis doesn't use the fact that different events happened during those 240 seconds. Specifically, during those 240 seconds, the zebrafish was presented with 12 different direction-specific visual patterns, with each one lasting for 20 seconds, for a total of 12 x 20 = 240 features. Stronger patterns are likely to emerge if we incorporate knowledge of our experimental setup into our analysis.  As we'll see, we can isolate the impact of temporal response or direction-specific impact by appropriately aggregating our features.\n",
    "\n",
    "In order to aggregate the features we will use basic ideas from matrix multiplication.  First, note that if we use `np.dot` with a two-dimensional array, then NumPy performs the equivalent matrix-multiply calculation.  For example, `np.array([[1, 2, 3], [4, 5, 6]]).dot(np.array([2, 0, 1]))` produces `np.array([5, 14])`.\n",
    "\n",
    "\\\\[\\begin{bmatrix} 1 & 2 & 3 \\\\\\ 4 & 5 & 6 \\end{bmatrix} \\begin{bmatrix} 2 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 14 \\end{bmatrix} \\\\]\n",
    "\n",
    "By setting up our multi-dimensional array properly we can multiply it by a vector to perform certain aggregation operations.  For example, imagine we had a 3 dimensional vector, \\\\( \\scriptsize \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}^\\top \\\\)  and we wanted to create a 2 dimensional vector containing the sum of its first and last elements as one value and three times its second value as another value, i.e., \\\\( \\scriptsize \\begin{bmatrix} 4 & 6 \\end{bmatrix}^\\top \\\\). We can generate this result via matrix multiplication as follows: `np.array([[1, 0, 1], [0, 3, 0]]).dot(np.array([1, 2, 3])` which produces `np.array([4, 6]`.\n",
    "\n",
    "\\\\[\\begin{bmatrix} 1 & 0 & 1 \\\\\\ 0 & 3 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\\\ 2 \\\\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 6 \\end{bmatrix} \\\\]\n",
    "\n",
    "For this exercise, you'll create several arrays that perform different types of aggregation.  The aggregation is specified in the comments before each array.  You should fill in the array values by hand.  We'll automate array creation in the next two exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63e0b680-5bbf-4a37-abdb-63d32600a103",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac125a9fdac6a5971c03f744d236a1e2",
     "grade": false,
     "grade_id": "answer_aggregationArray4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to create arrays for different types of arregation\n",
    "vector = np.array([0., 1., 2., 3., 4., 5.])\n",
    "\n",
    "# # Create a multi-dimensional array that when multiplied (using .dot) against vector, results in\n",
    "# # a two element array where the first element is the sum of the 0, 2, and 4 indexed elements of\n",
    "# # vector and the second element is the sum of the 1, 3, and 5 indexed elements of vector.\n",
    "# # This should be a 2 row by 6 column array\n",
    "\n",
    "# sum_every_other = np.array(<FILL_IN)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# # Create a multi-dimensional array that when multiplied (using .dot) against vector, results in a\n",
    "# # three element array where the first element is the sum of the 0 and 3 indexed elements of vector,\n",
    "# # the second element is the sum of the 1 and 4 indexed elements of vector, and the third element is\n",
    "# # the sum of the 2 and 5 indexed elements of vector.\n",
    "# # This should be a 3 row by 6 column array\n",
    "\n",
    "# sum_every_third = np.array(<FILL_IN)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# # Create a multi-dimensional array that can be used to sum the first three elements of vector and\n",
    "# # the last three elements of vector, which returns a two element array with those values when dotted\n",
    "# # with vector.\n",
    "# # This should be a 2 row by 6 column array\n",
    "\n",
    "# sum_by_three = np.array(<FILL_IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# # Create a multi-dimensional array that that sums the first two elements, second two elements, and\n",
    "# # last two elements of vector, which returns a three element array with those values when dotted\n",
    "# # with vector.\n",
    "# # This should be a 3 row by 6 column array\n",
    "# sum_by_two = np.array(<FILL_IN>)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print ('sum_every_other.dot(vector):\\t{0}'.format(sum_every_other.dot(vector)))\n",
    "print ('sum_every_third.dot(vector):\\t{0}'.format(sum_every_third.dot(vector)))\n",
    "\n",
    "print ('\\nsum_by_three.dot(vector):\\t{0}'.format(sum_by_three.dot(vector)))\n",
    "print ('sum_by_two.dot(vector): \\t{0}'.format(sum_by_two.dot(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1bf90f3-88ff-4495-bb8c-6cdf948200bd",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfaefe7666b99f1ee2f77a4f05bb775b",
     "grade": true,
     "grade_id": "test_aggregationArray4a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Aggregation using arrays (4a)\n",
    "assert_equal(sum_every_other.shape, (2, 6), 'incorrect shape for sum_every_other')\n",
    "assert_equal(sum_every_third.shape, (3, 6), 'incorrect shape for sum_every_third')\n",
    "assert_true(np.allclose(sum_every_other.dot(vector), [6, 9]), 'incorrect value for sum_every_other')\n",
    "assert_true(np.allclose(sum_every_third.dot(vector), [3, 5, 7]),\n",
    "                'incorrect value for sum_every_third')\n",
    "assert_equal(sum_by_three.shape, (2, 6), 'incorrect shape for sum_by_three')\n",
    "assert_equal(sum_by_two.shape, (3, 6), 'incorrect shape for sum_by_two')\n",
    "assert_true(np.allclose(sum_by_three.dot(vector), [3, 12]), 'incorrect value for sum_by_three')\n",
    "assert_true(np.allclose(sum_by_two.dot(vector), [1, 5, 9]), 'incorrect value for sum_by_two')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35583d0-4f79-482c-8f37-e5323a62890d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (4b) Recreate with `np.tile` and `np.eye`\n",
    "[np.tile](http://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html) is useful for repeating arrays in one or more dimensions.  For example, `np.tile(np.array([[1, 2], [3, 4]]), 2)` produces `np.array([[1, 2, 1, 2], [3, 4, 3, 4]]))`.\n",
    "\n",
    " \\\\[ np.tile( \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} , 2) \\to \\begin{bmatrix} 1 & 2 & 1& 2 \\\\\\ 3 & 4 & 3 & 4 \\end{bmatrix} \\\\]\n",
    "\n",
    "Recall that [np.eye](http://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html) can be used to create an identity array \\\\( (\\mathbf{I_n}) \\\\).  For example, `np.eye(3)` produces `np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])`.\n",
    "\n",
    "\\\\[ np.eye( 3 ) \\to \\begin{bmatrix} 1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1 \\end{bmatrix} \\\\]\n",
    "\n",
    "In this exercise, recreate `sum_every_other` and `sum_every_third` using `np.tile` and `np.eye`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d00c128c-6b2e-4ab1-87c1-7be0bbaf59c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reference for what to recreate\n",
    "print ('sum_every_other: \\n{0}'.format(sum_every_other))\n",
    "print ('\\nsum_every_third: \\n{0}'.format(sum_every_third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac9a04e4-9745-46ff-82dd-8a5c78b8f0d3",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70239b8b26e6676cd7c55cb2802c7049",
     "grade": false,
     "grade_id": "answer_tileAggregation4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to use np.tile and np.eye to recreate the arrays\n",
    "\n",
    "# sum_every_other_tile = <FILL_IN>\n",
    "# sum_every_third_tile = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print (sum_every_other_tile)\n",
    "print ('sum_every_other_tile.dot(vector): {0}'.format(sum_every_other_tile.dot(vector)))\n",
    "print ('\\n', sum_every_third_tile)\n",
    "print ('sum_every_third_tile.dot(vector): {0}'.format(sum_every_third_tile.dot(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b801f4bb-bbca-4316-8b15-62800f54bc99",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a28c6d348b517dfdd6be26eb0ebc718",
     "grade": true,
     "grade_id": "test_tileAggregation4b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Recreate with `np.tile` and `np.eye` (4b)\n",
    "assert_equal(sum_every_other_tile.shape, (2, 6), 'incorrect shape for sum_every_other_tile')\n",
    "assert_equal(sum_every_third_tile.shape, (3, 6), 'incorrect shape for sum_every_third_tile')\n",
    "assert_true(np.allclose(sum_every_other_tile.dot(vector), [6, 9]),\n",
    "                'incorrect value for sum_every_other_tile')\n",
    "assert_true(np.allclose(sum_every_third_tile.dot(vector), [3, 5, 7]),\n",
    "                'incorrect value for sum_every_third_tile')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bdf3a96-385e-40e1-a203-7d3567d6035b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (4c) Recreate with `np.kron`\n",
    "The Kronecker product is the generalization of outer products involving matrices, and we've included some examples below to illustrate the idea.  Please refer to the [Wikipedia page](https://en.wikipedia.org/wiki/Kronecker_product) for a detailed definition.  We can use [np.kron](http://docs.scipy.org/doc/numpy/reference/generated/numpy.kron.html) to compute Kronecker products and recreate the `sum_by` arrays.  Note that \\\\( \\otimes \\\\) indicates a Kronecker product.\n",
    "\n",
    "\\\\[ \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 & 2 \\end{bmatrix}  = \\begin{bmatrix} 1 \\cdot 1 & 1 \\cdot 2 & 2 \\cdot 1 & 2 \\cdot 2 \\\\\\ 3 \\cdot 1 & 3 \\cdot 2 & 4 \\cdot 1 & 4 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 2 & 4 \\\\\\ 3 & 6 & 4 & 8 \\end{bmatrix}  \\\\]\n",
    "\n",
    "We can see how the Kronecker product continues to expand if there are multiple rows in the second array.\n",
    "\n",
    "\\\\[ \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} \\otimes \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} & 2 \\cdot \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} \\\\\\ \\\\\\ 3 \\cdot \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} & 4 \\cdot \\begin{bmatrix} 5 & 6 \\\\\\ 7 & 8 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 5 & 6 & 10 & 12 \\\\\\ 7 & 8 & 14 & 16 \\\\\\ 15 & 18 & 20 & 24 \\\\\\ 21 & 24 & 28 & 32 \\end{bmatrix} \\\\]\n",
    "\n",
    "For this exercise, you'll recreate the `sum_by_three` and `sum_by_two` arrays using `np.kron`, `np.eye`, and `np.ones`.  Note that `np.ones` creates an array of all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef772292-ceee-48b7-b4f4-1499286055ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reference for what to recreate\n",
    "print ('sum_by_three: \\n{0}'.format(sum_by_three))\n",
    "print ('\\nsum_by_two: \\n{0}'.format(sum_by_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e25ad87-c3aa-44bb-9035-822dfb58d812",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3713f0104aaae7ed05d3efd7cfbb46e",
     "grade": false,
     "grade_id": "answer_kronAggregation4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to use np.kron, np.eye, and np.ones to recreate the arrays\n",
    "\n",
    "# sum_by_three_kron = <FILL_IN>\n",
    "# sum_by_two_kron = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print (sum_by_three_kron)\n",
    "print ('sum_by_three_kron.dot(vector): {0}'.format(sum_by_three_kron.dot(vector)))\n",
    "print ('\\n', sum_by_two_kron)\n",
    "print ('sum_by_two_kron.dot(vector): {0}'.format(sum_by_two_kron.dot(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "310cb015-db15-4dc0-a2e9-f995e5e168e4",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dea919c64531806eaa5691da674b46a7",
     "grade": true,
     "grade_id": "test_kronAggregation4c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Recreate with `np.kron` (4c)\n",
    "assert_equal(sum_by_three_kron.shape, (2, 6), 'incorrect shape for sum_by_three_kron')\n",
    "assert_equal(sum_by_two_kron.shape, (3, 6), 'incorrect shape for sum_by_two_kron')\n",
    "assert_true(np.allclose(sum_by_three_kron.dot(vector), [3, 12]),\n",
    "                'incorrect value for sum_by_three_kron')\n",
    "assert_true(np.allclose(sum_by_two_kron.dot(vector), [1, 5, 9]),\n",
    "                'incorrect value for sum_by_two_kron')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eb15507-33d5-4087-a6c9-adf22dd29314",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (4d) Aggregate by time\n",
    "\n",
    "As we discussed in Part (4a), we would like to incorporate knowledge of our experimental setup into our analysis. To do this, we'll first study the temporal aspects of neural response, by aggregating our features by time. In other words, we want to see how different pixels (and the underlying neurons captured in these pixels) react in each of the 20 seconds after a new visual pattern is displayed, regardless of what the pattern is.  Hence, instead of working with the 240 features individually, we'll aggregate the original features into 20 new features, where the first new feature captures the pixel response one second after a visual pattern appears, the second new feature is the response after two seconds, and so on.\n",
    "\n",
    "We can perform this aggregation using a map operation. First, build a multi-dimensional array \\\\( \\scriptsize \\mathbf{T} \\\\) that, when dotted with a 240-component vector, sums every 20-th component of this vector and returns a 20-component vector. Note that this exercise is similar to (4b).  Once you have created your multi-dimensional array \\\\( \\scriptsize \\mathbf{T} \\\\), use a `map` operation with that array and each time series to generate a transformed dataset. We'll cache and count the output, as we'll be using it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "223b299b-7ca6-4bcf-b13c-2a8a2a79ba87",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12ec8efcbdb7b9429bbf2b69b7b8fe64",
     "grade": false,
     "grade_id": "answer_timeAggregation4d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to create a multi-dimensional array to perform the aggregation\n",
    "# # Create a multi-dimensional array to perform the aggregation\n",
    "# T = <FILL_IN>\n",
    "\n",
    "# # Transform scaled_data using T.  Make sure to retain the keys.\n",
    "# time_data = scaled_data.<FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "time_data.cache()\n",
    "print (time_data.count())\n",
    "print (time_data.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "275ee8bb-4e25-4a08-9696-f890f29a6b04",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfff9dfc0509065d6a2e9a951a8ac7ac",
     "grade": true,
     "grade_id": "test_timeAggregation4d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Aggregate by time (4d)\n",
    "assert_equal(T.shape, (20, 240), 'incorrect shape for T')\n",
    "time_data_first = time_data.values().first()\n",
    "time_data_fifth = time_data.values().take(5)[4]\n",
    "assert_equal(time_data.count(), 46460, 'incorrect length of time_data')\n",
    "assert_equal(time_data_first.size, 20, 'incorrect value length of time_data')\n",
    "assert_equal(time_data.keys().first(), (0, 0), 'incorrect keys in time_data')\n",
    "assert_true(np.allclose(time_data_first[:2], [0.00802155, 0.00607693], atol=1e-2),\n",
    "                'incorrect values in time_data')\n",
    "assert_true(np.allclose(time_data_fifth[-2:], [-0.00636676, -0.0179427], atol=1e-2),\n",
    "                'incorrect values in time_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "944284ca-8ed5-48ee-8d71-23cae5822fea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (4e) Obtain a compact representation\n",
    "\n",
    "We now have a time-aggregated dataset with \\\\(\\scriptsize n = 46460\\\\) pixels and \\\\(\\scriptsize d = 20\\\\) aggregated time features, and we want to use PCA to find a more compact representation.  Use the `pca` function from Part (2a) to perform PCA on the this data with \\\\(\\scriptsize k = 3\\\\), resulting in a new low-dimensional 46,460 by 3 dataset. As before, you'll need to extract the values from `time_data` since it is an RDD of key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33925087-956d-4bc3-bb71-b66fb956bbf0",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6deab270209c62a1aba53797a154bb36",
     "grade": false,
     "grade_id": "answer_compactRep4e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to extract a compact representation from time_data using pca function from Part (2a)\n",
    "\n",
    "# components_time, time_scores, eigenvalues_time = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print ('components_time: (first five) \\n{0}'.format(components_time[:5, :]))\n",
    "print ('\\ntime_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, time_scores.take(3)))))\n",
    "print ('\\neigenvalues_time: (first five) \\n{0}'.format(eigenvalues_time[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26ea687-99b6-4be4-b2ec-1bad207c39ef",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc32da6c7c84bc61ace70661be8904af",
     "grade": true,
     "grade_id": "test_compactRep4e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Obtain a compact representation (4e)\n",
    "assert_equal(components_time.shape, (20, 3), 'incorrect shape for components_time')\n",
    "assert_true(np.allclose(np.abs(np.sum(components_time[:5, :])), 2.37299020, atol=1e-2),\n",
    "                'incorrect value for components_time')\n",
    "assert_true(np.allclose(np.abs(np.sum(time_scores.take(3))), 0.0213119114, atol=1e-2),\n",
    "                'incorrect value for time_scores')\n",
    "assert_true(np.allclose(np.sum(eigenvalues_time[:5]), 0.844764792, atol=1e-2),\n",
    "                'incorrect value for eigenvalues_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b5c113-07ee-4ac6-891c-5b902ed5d833",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 9: Top two components by time\n",
    "\n",
    "Let's view the scores from the first two principal components as a composite image. When we preprocess by aggregating by time and then perform PCA, we are only looking at variability related to temporal dynamics. As a result, if neurons appear similar -- have similar colors -- in the resulting image, it means that their responses vary similarly over time, regardless of how they might be encoding direction. In the image below, we can define the midline as the horizontal line across the middle of the brain.  We see clear patterns of neural activity in different parts of the brain, and crucially note that the regions on either side of the midline are similar, which suggests that temporal dynamics do not differ across the two sides of the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeec66ea-e73f-497a-ad7f-8212cb733feb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scores_time = np.vstack(time_scores.collect())\n",
    "image_one_time = scores_time[:, 0].reshape(230, 202).T\n",
    "image_two_time = scores_time[:, 1].reshape(230, 202).T\n",
    "brainmap = polar_transform(3, [image_one_time, image_two_time])\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap,interpolation='nearest', aspect='auto')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bd4d277-080e-457b-9678-0e1163ebcbef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (4f) Aggregate by direction\n",
    "\n",
    "Next, let's perform a second type of feature aggregation so that we can study the direction-specific aspects of neural response, by aggregating our features by direction. In other words, we want to see how different pixels (and the underlying neurons captured in these pixels) react when the zebrafish is presented with 12 direction-specific patterns, ignoring the temporal aspect of the reaction.  Hence, instead of working with the 240 features individually, we'll aggregate the original features into 12 new features, where the first new feature captures the average pixel response to the first direction-specific visual pattern, the second new feature is the response to the second direction-specific visual pattern, and so on.\n",
    "\n",
    "As in Part (4c), we'll design a multi-dimensional array \\\\( \\scriptsize \\mathbf{D} \\\\) that, when multiplied by a 240-dimensional vector, sums the first 20 components, then the second 20 components, and so on. Note that this is similar to exercise (4c).  First create \\\\( \\scriptsize \\mathbf{D} \\\\), then use a `map` operation with that array and each time series to generate a transformed dataset. We'll cache and count the output, as we'll be using it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b98546bf-b24a-484c-b840-bc73054703c9",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "258004df72713a9dc69145202b4539b3",
     "grade": false,
     "grade_id": "answer_directionAggregation4f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to create a multi-dimensional array to perform the aggregation\n",
    "# D = <FILL_IN>\n",
    "\n",
    "# # Transform scaled_data using D.  Make sure to retain the keys.\n",
    "# direction_data = scaled_data. <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "direction_data.cache()\n",
    "print (direction_data.count())\n",
    "print (direction_data.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a56106bd-38cd-4f92-b6cc-b6e62e9df9de",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "409e318e6f53505b8267fb21eef9b7cc",
     "grade": true,
     "grade_id": "test_directionAggregation4f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Aggregate by direction (4f)\n",
    "assert_equal(D.shape, (12, 240), 'incorrect shape for D')\n",
    "direction_data_first = direction_data.values().first()\n",
    "direction_data_fifth = direction_data.values().take(5)[4]\n",
    "assert_equal(direction_data.count(), 46460, 'incorrect length of direction_data')\n",
    "assert_equal(direction_data_first.size, 12, 'incorrect value length of direction_data')\n",
    "assert_equal(direction_data.keys().first(), (0, 0), 'incorrect keys in direction_data')\n",
    "assert_true(np.allclose(direction_data_first[:2], [0.03346365, 0.03638058], atol=1e-2),\n",
    "                'incorrect values in direction_data')\n",
    "assert_true(np.allclose(direction_data_fifth[:2], [0.01479147, -0.02090099], atol=1e-2),\n",
    "                'incorrect values in direction_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78d41a91-c422-432c-aba0-a22b51a10ef6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (4g) Compact representation of direction data\n",
    "\n",
    "We now have a direction-aggregated dataset with \\\\(\\scriptsize n = 46460\\\\) pixels and \\\\(\\scriptsize d = 12\\\\) aggregated direction features, and we want to use PCA to find a more compact representation.  Use the `pca` function from Part (2a) to perform PCA on the this data with \\\\(\\scriptsize k = 3\\\\), resulting in a new low-dimensional 46460 by 3 dataset. As before, you'll need to extract the values from `direction_data` since it is an RDD of key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ae1e34-e22e-4a21-9faf-3dfef093af19",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e989a66ebe173955b1cfa1b42be03169",
     "grade": false,
     "grade_id": "answer_compactDirection4g",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment the lines below and replace <FILL_IN> with appropriate code to get compact representation of direction data using pca function\n",
    "# components_direction, direction_scores, eigenvalues_direction = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print ('components_direction: (first five) \\n{0}'.format(components_direction[:5, :]))\n",
    "print ('\\ndirection_scores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, direction_scores.take(3)))))\n",
    "print ('\\neigenvalues_direction: (first five) \\n{0}'.format(eigenvalues_direction[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e56eaef-4424-4505-a6ce-8f0eb602fc82",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a97706959c379e47dfadd3a6469e5d5",
     "grade": true,
     "grade_id": "test_compactDirection4g",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Compact representation of direction data (4g)\n",
    "assert_equal(components_direction.shape, (12, 3), 'incorrect shape for components_direction')\n",
    "assert_true(np.allclose(np.abs(np.sum(components_direction[:5, :])), 1.080232069, atol=1e-2),\n",
    "                'incorrect value for components_direction')\n",
    "assert_true(np.allclose(np.abs(np.sum(direction_scores.take(3))), 0.10993162084, atol=1e-2),\n",
    "                'incorrect value for direction_scores')\n",
    "assert_true(np.allclose(np.sum(eigenvalues_direction[:5]), 2.0089720377, atol=1e-2),\n",
    "                'incorrect value for eigenvalues_direction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ba5c9a-0fc5-40c1-9843-191d6df6e45d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualization 10: Top two components by direction\n",
    "\n",
    "Again, let's view the scores from the first two principal components as a composite image.  When we preprocess by averaging across time (group by direction), and then perform PCA, we are only looking at variability related to stimulus direction. As a result, if neurons appear similar -- have similar colors -- in the image, it means that their responses vary similarly across directions, regardless of how they evolve over time. In the image below, we see a different pattern of similarity across regions of the brain.  Moreover, regions on either side of the midline are colored differently, which suggests that we are looking at a property, direction selectivity, that has a different representation across the two sides of the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3199116-a947-4a28-88ed-5d3d500c1041",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scores_direction = np.vstack(direction_scores.collect())\n",
    "image_one_direction = scores_direction[:, 0].reshape(230, 202).T\n",
    "image_two_direction = scores_direction[:, 1].reshape(230, 202).T\n",
    "brainmap = polar_transform(2, [image_one_direction, image_two_direction])\n",
    "# with thunder: Colorize(cmap='polar', scale=2).transform([image_one_direction, image_two_direction])\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hide_labels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap, interpolation='nearest', aspect='auto')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddb1a19e-4287-4fbd-98b0-2436499d2f26",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "## Part 5: Random-projection and Johnson–Lindenstrauss lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd6cf8c8-4842-43e1-8e33-f3f894aa002b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (5a) Johnson–Lindenstrauss lemma\n",
    "\n",
    "[Random projection](https://en.wikipedia.org/wiki/Random_projection) is a strategy used to reduce the dimensionality of a set of points that lie in Euclidean space. To better understand random projection, we'll be working with the extracted values of the preprocessed neuroscience data (scaled_data_extracted). In this exercise, we will be randomly projecting the number of samples, the left side of the data matrix, to a smaller dimension d. \n",
    "\n",
    "[Johnson–Lindenstrauss lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma) states that points in a high-dimensional space can be embedded into a lower dimensional subspace where the pairwise distances between data points are nearly preserved. There exists the smallest dimension d that we can use for the the dimension of the lower dimensional subspace, where the pairwise distance wouldn't change more than (1+ epsilon) factor. Given there are n data points,  d is defined as below:\n",
    "$$d = O( \\frac{4 \\log(n)}{\\epsilon^2/2-\\epsilon^3/3})$$\n",
    "\n",
    "In 5(a) we will calculate the d of the preprocessed neuroscience dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef1e07b-3107-4654-b063-ca0b2607a0f9",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c49d28acc54a3c2311e27a74b913058f",
     "grade": false,
     "grade_id": "answer_jl_d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n",
    "# # get the dimensions of the preprocessed neuroscience data \n",
    "# # n is the number of rows and k is the number of columns in the dataset\n",
    "# n = <FILL IN>\n",
    "# k = <FILL IN>\n",
    "\n",
    "# # calculate d \n",
    "# eps = 0.1\n",
    "# d = <FILL_IN>\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# We will increase d a bit to ensure that the random projection \n",
    "# has more than enough dimensions to maintian pairwise distance\n",
    "d += 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db87f535-4b6c-4a85-b46d-2414a28ff254",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c0a275dceba9316e769844768361fc2",
     "grade": true,
     "grade_id": "test_jl_d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST Johnson–Lindenstrauss lemma (5a)\n",
    "assert_equal(n, 46460, \"Incorrect number of samples\")\n",
    "assert_equal(k, 240, \"Incorrect number of features\")\n",
    "assert_equal(d, 9411, \"Incorrect calculation of d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30c7c92c-07ad-4ea8-b9c3-f3d23e441c84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (5b) Random projections\n",
    "Now that we know the smallest dimension d that we can use for the dimension of the lower dimensional subspace, we can create randomly projected matrix of our preprocessed neuroscience dataset. Given that the neurosicence dataset is an n by k matrix (n samples and k features), we will create a randomly projected matrix of size d by k. The randomly projected matrix can be computed by multiplying a random gaussian matrix of size d by n to the original dataset. Each element of the random gaussian matrix is sampled from a standard normal distribution N(0,1) and scaled by sqrt(1/n). To find an element in the random gaussian matrix, you will sample a random point in standard normal distribution and then multiply it by sqrt(1/n). To ensure reproducibility, we will be using [numpy random generator](https://numpy.org/doc/stable/reference/random/generator.html) to generate random numbers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce45432-b315-487a-99fd-a055840c66ee",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b231fc1605c6036372f45dfd6b212c8e",
     "grade": false,
     "grade_id": "answer_random_projection",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the following lines and replace <FILL_IN> with appropriate code\n",
    "# # Compute the project matrix by taking the dot product between the dataset and a random matrix of size d x n\n",
    "# # HINT: Take the dot product of the data and random matrix by using map and taking the sum of the outer products - similar to 1(b)\n",
    "# def compute_random_projection(data, d):\n",
    "#     \"\"\"Calculates the projected rdd matrix that will be used to project the dataset to the d dimension\n",
    "    \n",
    "#     Note: \n",
    "#     The 'dot' product of two matrices can be calculated in a distributive manner by taking the sum of the outer products. The function for the outer product is np.outer(). \n",
    "    \n",
    "#     Args: \n",
    "#     data (RDD of extracted values) : The dataset matrix to which we will project\n",
    "#     d (int) : the lower dimensional level\n",
    "    \n",
    "#     Returns:\n",
    "#     PythonRDD: An RDD of values representing the matrix containing the points of the dataset projected to the d dimension. \n",
    "#     \"\"\"\n",
    "\n",
    "#     # Use rng.normal(size=(x,y)) to generate a matrix of random numbers of size x by y \n",
    "#     # from a standard normal distribution\n",
    "#     rng = np.random.default_rng(123)\n",
    "#     n = <FILL_IN>\n",
    "#     k = <FILL_IN>\n",
    "#     projected_matrix = data.map(<FILL_IN>)\n",
    "#     projected_rdd = sc.parallelize(projected_matrix)\n",
    "#     raise NotImplementedError()\n",
    "#     return projected_rdd\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Compute the random projected rdd matrix\n",
    "# Note: calculating the random projected matrix may take a few minutes\n",
    "projected_rdd = compute_random_projection(scaled_data, d)\n",
    "projected_rdd_count = projected_rdd.count()\n",
    "projected_rdd_cols = len(projected_rdd.first())\n",
    "projected_rdd_scaling = projected_rdd.map(lambda x: abs(x[0])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e450075-e7a5-4d72-b1fd-3d8374ae315a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TEST Random projection (5b)\n",
    "assert_equal(projected_rdd_count, 9411, \"Incorrect number of rows in random projection\")\n",
    "assert_equal(len(projected_rdd_cols), 240, \"Incorrect number of columns in random projection\")\n",
    "\n",
    "assert_true(np.allclose(projected_rdd_scaling, 240, atol=10), \"Incorrect scaling of random projection\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3d8ca08-bab5-49f0-aabf-c1b4021a5bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (5c) Comparing eigenvalues\n",
    "We will use the pca function we wrote before to calculate the eigenvalues of the random projection and the pre-processed neuroscience data (scaled_data_extracted). If the lemma holds, the top eigen values of the random projection should be similar to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2925f8a-2bea-453a-bc5a-7fa23ca0b597",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e581a472d51b09f9e03533bdac2e16d",
     "grade": false,
     "grade_id": "answer_jl_eigen",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TODO: Uncomment the following lines and replace <FILL_IN> with appropriate code\n",
    "# #find the eigenvalues of the preprocessed neuroscience data\n",
    "# eigenvectors, score, eigenvalues = <FILL_IN>\n",
    "\n",
    "# #find the eigenvalues of the random projection\n",
    "# eigenvectors_projected, score_projected, eigenvalues_projected = <FILL_IN>\n",
    "\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d223ebb7-b828-4950-93c1-e0a3503b60af",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e992323069e7a91e88b8ef3474b0db7",
     "grade": true,
     "grade_id": "test_jl_eigen",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigenvalues_gold = [0.10891779, 0.05496768, 0.02565018, 0.00935326, 0.0080986, 0.00542128, 0.00363197, 0.00308395, 0.00244984, 0.00204938]\n",
    "eigenvalues_projected_gold = [0.16831054, 0.06407432, 0.03105165, 0.00934892, 0.00823222, 0.00494922, 0.00422611, 0.00299032, 0.00257829, 0.00204315]\n",
    "assert_true(np.allclose(eigenvalues[:10], eigenvalues_gold, atol=0.001), \"Incorrect eigenvalues in original dataset\" )\n",
    "assert_true(np.allclose(eigenvalues_projected[:10], eigenvalues_projected_gold, atol=0.08), \"Incorrect eigenvalues in projected dataset\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae944c20-3f18-4f9b-8feb-c832dd206778",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Visualization 11: Top two eigenvectors of original data and random projection\n",
    "\n",
    "Let's visualize the top two eigenvectors of the original data and the top two eigenvectors of the random projection. If computed properly, the eigenvectors of the original data and the random projection should be similar. To visualize the eigenvectors, each element of the top two eigenvectors will be plotted as individual points. For example, one data point would be (first element of the first eigenvector, first element of the second eigenvector). Blue points represent the eigenvectors of the orginal data and green points represent the eigenvectors of the random projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7adad908-ea16-40ed-bbb5-85f96f669d42",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = prepare_plot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'elements of the first eigenvector'), ax.set_ylabel(r'elements of the second eigenvector')\n",
    "plt.scatter(abs(np.transpose(eigenvectors)[0]), abs(np.transpose(eigenvectors)[1]), c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(abs(np.transpose(eigenvectors_projected)[0]), abs(np.transpose(eigenvectors_projected)[1]), c='#62c162', alpha=.75)\n",
    "display(fig)\n",
    "\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hw2_part2_new",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
